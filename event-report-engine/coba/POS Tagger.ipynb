{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/titut/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/titut/.local/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import html\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import anago\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66378098</td>\n",
       "      <td>[HARI PEREMPUAN INTERNASIONAL]\\n\\nHari Perempu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66378098</td>\n",
       "      <td>Himasika mengucapkan selamat hari raya idul Ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66378098</td>\n",
       "      <td>[COMING SOON]\\n\\nHalo mahasiswa fisika!\\nMusya...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66378098</td>\n",
       "      <td>[PENDAFTARAN BMS 2018 DIBUKA]\\n\\nPendaftaran B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66378098</td>\n",
       "      <td>Persiapan hari ke 2 OKKBK Fisika ITS semangat ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id                                              tweet  label\n",
       "0  66378098  [HARI PEREMPUAN INTERNASIONAL]\\n\\nHari Perempu...      0\n",
       "1  66378098  Himasika mengucapkan selamat hari raya idul Ad...      0\n",
       "2  66378098  [COMING SOON]\\n\\nHalo mahasiswa fisika!\\nMusya...      0\n",
       "3  66378098  [PENDAFTARAN BMS 2018 DIBUKA]\\n\\nPendaftaran B...      0\n",
       "4  66378098  Persiapan hari ke 2 OKKBK Fisika ITS semangat ...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/tweets.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ner(text):\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r'\\w+:\\/\\/\\S+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = os.linesep.join([s for s in text.splitlines() if s])\n",
    "    text = ', '.join(text.split('\\n'))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slang(text, acronym):\n",
    "    res = []\n",
    "    \n",
    "    for w in text.split(' '):\n",
    "        if w in acronym:\n",
    "            res.append(acronym[w])\n",
    "        else:\n",
    "            res.append(w)\n",
    "    \n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extra/akronim.json\", \"r\") as f:    \n",
    "    acronym_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_ner'] = data['tweet'].apply(lambda x: clean_text_ner(x))\n",
    "data['tweet_ner'] = data['tweet_ner'].apply(lambda x: replace_slang(x, acronym_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mrrizal/POS_Tag_Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('pos-tag-indonesian/indonesian_ngram_pos_tag.pickle', 'rb')\n",
    "ngram_tagger = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "isascii = lambda s: len(s) == len(s.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(text):\n",
    "    global ngram_tagger\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    \n",
    "    res_tag = ngram_tagger.tag(word_tokenize(text))\n",
    "    for x in res_tag:\n",
    "        if x[0] in string.punctuation:\n",
    "            words.append(x[0])\n",
    "            tags.append('Z')\n",
    "        elif not isascii(x[0]):\n",
    "            words.append(x[0])\n",
    "            tags.append('EMO')\n",
    "        else:\n",
    "            words.append(x[0])\n",
    "            tags.append(x[1])\n",
    "            \n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "with open('extra/Gazetteer.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        places.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_gazetteer(text):\n",
    "    global places\n",
    "    \n",
    "    place, score = process.extractOne(text, places, scorer=fuzz.token_sort_ratio)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_position(text, query):\n",
    "    s1 = '<ENAMEX TYPE=\"\">'\n",
    "    s2 = '</ENAMEX>'\n",
    "    pos = text.find(query)\n",
    "    new_text = text[:pos] + s1 + text[pos:pos+len(s1)] + s2 + text[pos+len(s1):]\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_label(text, tweet):\n",
    "    words, tags = pos_tag(text)\n",
    "    idx_s = 0\n",
    "    idx_e = 0\n",
    "    \n",
    "    cur_s = 0\n",
    "    cur_e = 0\n",
    "    cur_score = 0\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        if(tags[i] == 'NN' or tags[i] == 'NNP'):\n",
    "            if(i == 0):\n",
    "                idx_s = 0\n",
    "                idx_e = 0\n",
    "            elif(tags[i-1] == 'NN' or tags[i-1] == 'NNP'):\n",
    "                idx_e = i\n",
    "            else:\n",
    "                idx_s = i\n",
    "                idx_e = i\n",
    "        elif(i != 0 and (tags[i-1] == 'NN' or tags[i-1] == 'NNP')):\n",
    "            if(idx_e - idx_s + 1 > 1):\n",
    "                query = ' '.join(words[idx_s:idx_e+1])\n",
    "                tweet = get_string_position(tweet, query)\n",
    "#                 score = match_gazetteer(query)\n",
    "#                 if score > cur_score:\n",
    "#                     cur_score = score\n",
    "#                     cur_s = idx_s\n",
    "#                     cur_e = idx_e\n",
    "        \n",
    "        if((i == len(tags)-1) and (tags[i] == 'NN' or tags[i] == 'NNP')):\n",
    "            if(idx_e - idx_s + 1 > 1):\n",
    "                query = ' '.join(words[idx_s:idx_e+1])\n",
    "                tweet = get_string_position(tweet, query)\n",
    "#                 score = match_gazetteer(query)\n",
    "#                 if score > cur_score:\n",
    "#                     cur_score = score\n",
    "#                     cur_s = idx_s\n",
    "#                     cur_e = idx_e\n",
    "\n",
    "        \n",
    "#     for x in range(cur_s, cur_e+1):\n",
    "#         tags[x] = \"LOC\"\n",
    "    \n",
    "#     for i in range(len(tags)):\n",
    "#         if(tags[i] == 'LOC'):\n",
    "#             tags[i] = 'B-LOC'\n",
    "#         else:\n",
    "#             tags[i] = 'O'\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = get_ner_label(data[data['label'] == 1].iloc[0]['tweet_ner'], data[data['label'] == 1].iloc[0]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ner_x = []\n",
    "data_ner_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Enamex.txt\", \"w\") as f:\n",
    "    for idx, row in data[data['label'] == 1].iterrows():\n",
    "        new_text = get_ner_label(row['tweet_ner'], row['tweet'])\n",
    "        f.write(new_text + '\\n-----------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ner_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bakor Pemandu ITS mengundang Pemandu Aktif ITS untuk duduk bareng ngobrolin LKMM TD pada:\\n\\nüìÜ Selasa - Rabu, 20-21 Feruari\\n2018\\nüïõ 18.00 - 21.30 WIB\\nüìç SCC Lt. 3\\nüëî Standar Kuliah\\n\\n\"Raise your standards to create change!\" - An Iota of Truth\\n\\n#OborBakor\\n#BAKORITS\\n#ITSSurabaya https://t.co/v6oFZcWLFv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label'] == 1].iloc[10]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
